---
layout: post
published: true
mathjax: false
featured: true
comments: true
title: TVQA Paper Review
categories:
  - DL
---
# TVQA

#### Localized, Compositional Video Question Answering



### Abstract

TVAQ : TV question-answer

dataset : 6 popular TV shows (Friends, The Big Bang Theory, ...)



### 1. Introduction

VQA : video question answer

questions 

* object identification
* counting
* appearance
* interactions
* social realationships......

image-based VAQ datasets : DAQUAR, COCO-QA, FM_IQA...

6개의 TV show에서 15만개의 human-written AQ pair을 수집함

dataset의 장점

* large-scale and natural(21793 video clips)
* clips are long(60-90sec)
* provide the dialogue (character name + subtitle)
* questions are compositional



### 2. Related Work

기존 VQA 의 문제점

 : truly realistic, multimodal QA scenario가 아님

visual and language understanding을 함께 사용하지 않고, 비현실적인 video source를 사용하거나, data collection에 더 힘을 쏟는 경향을 보임



### 3. TVQA Dataset

#### 3.1 Dataset Collection

6개의 TV show (3개의 장르 : 시트콤, 의학드라마, 범죄드라마)

총 461시간동안 925개의 에피소드가 있음

3달동안 사람들에게 하나의 비디오 클립당 1.3달러를 제공하고 각 비디오클립당 7개의 다른 질문들과, 틀린답변4개와 맞는 답변 1개를 만들어내도록 했다

#### 3.2 Dataset Analysis

맞는 답안은 주로 틀린 답안보다 조금 더 긴 것을 확인할 수 있었다.

### 4. Methods

multi-strea end-to-end trainable neural network for Multi-Modal Video Question Answering

 cross-validation을 통해 top6 regions를 선택하여 감지하여 수행함

감지한 사물의 label을 image input으로 사용하는 것이 성능을 높임

#### 4.2 LSTM Encoders for Video and Text

bi-directional LSTM

LSTM : long short-term memory : 가장 발전된 형태의 RNN 구조

subtitle S : contains set of sentences

S는 긴 단어의 연속으로 좁혀지고 GloVe가 워드 임베딩에 사용되었다. hidden state에 BiLSTM를 사용하였다. 양 방향으로 각 timestep 마다 subtitle을 얻기 위해 사용하였다. 

#### 4.3 Joint Modeling of Context and Query

context matching module과 BiLSTM은 contextual input을 만들기 위해 사용하였다.



### 5. Experiments

#### 5.1 Baselines

##### Longest Answer

정답은 평균적으로 길이가 더 길었기 때문에, 첫번째 baseline은 단순히 가장 긴 정답을 찾았다

##### Nearest Neighbor Search

NNS 는 question이나 subtitle로부터 가장 가까운 answer을 찾았다. TFIDF, SkipThought를 사용하여 문장을 임베딩하고, averaged GloVe 도 사용하였다

##### Retrieval

TVQA의 사이즈 때문에, 비슷한 질문들이 존재할 수 있었다. 그래서  가장 비슷한 question을 찾아서 사용하였다. 

#### 5.1 Results

##### Baseline Comparison

![result]({{site.baseurl}}/https://github.com/zimkjh/zimkjh.github.io/blob/master/images/post_images/2019-03-12-TVQA/capture1.PNG?raw=true)




### *** PororoQA

https://www.youtube.com/watch?v=EHiGjiP3FbY&feature=youtu.be

질의응답 에이전트 : 시리, Nugu

비디오 질의응답 에이전트 : 아이가 뽀로로 만화를 본다. 로봇도 만화를 시청한다. 그런 후에 아이가 로봇에게 질문을 하면 대답을 해준다.

비디오 하나에 scene과 dialogue 페어가 존재. 자막이 나올 때마다 화면을 캡쳐해서 결과적으로 한 영상에 나온 자막의 개수만큼의 데이터가 쌓임. 이상태로 문제를 해결하는 것은 몹시 어렵기 때문에 scene description이라는 것을 만든다.

scene description : 화면 설명문. 

화면을 설명하는 5개의 예시 문장이 있고, 그중에 무엇이 정답인지 맞추는 문제로 바꿔서 품

f_1 함수는 인자로 문장 하나를 받는다. 그리고 0에서 1 사이의 positive 인지 negative 인지를 출력한다.

##### 메모리가 필요한 순환 신경망

순환신경망 : recurrent Neural Network

sequence하게 학습하게 되는데 hidden layer에 있는 hidden vector에 인코딩을 시켜야 한다. hidden vector에 많은 데이터를 넣는데에 한계가 생긴다. 고정된 사이즈의 차원에 많은 정보를 넣는데는 한계가 있다. 

recurrent NN만 가지고 QA를 만들려면 단어단위로 모두 들어온 뒤에 hidden vector로 질의를 vector을 나타내서 답변을 생성해야 한다. hidden vector은 기사를 점점 학습함에 따라서 처음에 있는 문장의 information을 많이 까먹을것임.







### 참고

pororoQA

https://www.slideshare.net/NaverEngineering/qa-81412519
