# 자연어처리에 대한 이해 



## 기계학습을 이용한 대화 모델링



#### 지보 : MIT 연구실에서 만든 로봇

<http://smartramg.tistory.com/17>

2014년도에 2016에 시판하겠다고 했는데 아직도 안나옴
카메라, 스피커, 마이크가 달려있어서 문자 읽어주고 보내주고 사람 알아봐줌
내부적으로 이미지 처리 관련된 기술은 충분한데 자연어 처리가 안된다고 함
최근 딥러닝이 각광받은 이유는 이미지 관련 문제를 잘 풀었기 때문임.

#### 뽀로로 로봇

서울대 연구실에서 뽀로로 애니메이션으로 학습시킨 뽀로로 로봇에게 질문을 하는 영상

<https://youtu.be/OtkEkLpjs3s>

\+카페 주문받는 챗봇도 만듬 



## 규칙 기반 대화 모델링



#### Terry Winograd's Dissertation | 챗봇의 시초

3.png

MIT에서 AI 그룹에서 박사를 하면서 SHRDLU를 개발햇음
3차원 공간에 피라미드와 박스와 색깔이 있음
컴퓨터에 사람이 자연어로 명령을 내림. 박스를 어디로 옮겨
사람의 언어를 다 parsing, tagging해서 네모난건 변이 몇개, 이렇게 속성을 사람이 손으로 다 디자인 해서 설정해둠.
처리할 수 있는 단어 50개 정도
딥러닝이 아니라 모든것을 다 기억해서 내놓는 방식

Dialogue DEMO를 보면 무지 잘하는걸 볼 수 있음

1.png

##### 당시 기술로 이걸 어떻게 할 수 있었을까?

모든걸 손으로 다 태깅을 했음
물체가 3d world에 있을 수 있는 모든 지식을 체계화함.
어떤 형태의 질문이 들어오면 어떻게 파싱을 해야하는지, 행동은 어떻게 해야하는지 sequence를 사람이 다 짜놓음 -> 확장성이 없음
1960-70년대의 접근

여기 연구실에서는 SHRDLU(셔드루) 때의 이런 규칙들과 딥러닝을 결합해서 유연한 기능을 할 수 있는 자연어 처리 기법을 연구함
rule-based 챗봇은 이런식으로 돌아감카톡 플친 맺으면 돌아가는 챗봇들은 대부분 규칙기반으로 돌아가게 됨.
그래서 꼭 필요한 rule은 사람이 적어서 돌리고(다른사람들이 짜놓은게 있으니 새로 짤 필요가없음), 처음 보는 질문에 대해서는 define 해둘 수가 없으니 그때 딥러닝을 적용하는 추세 

#### ontology

2.png

이 가상환경에서 나올 법한 지식들지식 체계 : ontology지금까지는 사람들이 다 손으로 ontology로 만들어줬음 

## 확률 모델 기반 대화 모델링

- deterministic이 아닌 random 성을 주는 알고리즘

- Naive Bayes를 이용한 스팸 필터링

  - 4.png

- - 챗봇에 아주 들어맞는 확률기반 모델은 아니지만 자연어 처리에서 아주 유명한 확률 기반 모델
  - 모델을 세우고 모델에 대한 확률식을 세운 다음에, 그 모델에 있는 파라미터를 정의를 하고 그 모델에 있는 파라미터를 데이터를 통해서 학습을 시킴
  - 스팸메일에 자주나오는 단어들이있고, 스팸이 아닌 메일에 자주나오는 단어들이 있으니, 단어들의 개수를 이용해서 스팸인지 아닌지 판단하기
  - 확률적으로 문서에 나오는 단어의 빈도수를 이용해서 모델링을 함
  - 1970-80년대에 제안된 모델인데, 요새 사람들은 이렇게 하지 않지만 데이터가 많은 경우에는 아주 성능이 좋음

- 카페 주문 자연어처리

- - 카페 주문 환경에서는 사용자가 말 하는 것의 의도가 23가지 정도로 정의되었음

  - - 인사, 주문 계속, 주문 끝, 요청, 결제요청
    - 23가지의 의도를 tagging함
    - 모든 문장마다 tagging을 함

  - 한번 customer가 말을 하면 컴퓨터가 알아들을 수 있는 표현으로 바꾸고, 그 표현이 속하는 의도를 예측함.(23가지 중에서) 그다음 이 표현일때 그다음이 어떤 의도인지 확률을 학습을 해놔서 점원이 할 의도를 예측하고, 이걸 기반으로 점원이 할 말을 예측함

  - - topic classification

    - - 의도를 classification하는 것의 성능이 중요해서 SVM 쓰고, decision tree도 썼음
      - 의도 군집화와 그 사이는 모델링이 잘 됐는데, 그거에 맞는 문장을 만드는게 어려웠음. 확률모델이다보니 똑같은 질문에 따라 답변이 달라질 수도 있음

    - bow : bag of words 자료 링크

    - - <http://darkpgmr.tistory.com/125>

    - 정확도

    - - 의도 군집화는 bow와 svm만 해도 90퍼센트 넘게 손님의 주문의도나 안에서 있을 수 있는 의도를 예측함

- MDP 기반 대화 모델링

- - 딥러닝에 mdp 결합해서 요새 챗봇을 많이 생성하고 있음.

## 딥러닝을 이용한 자연어처리 기법

#### word Representation

언어라는 것은 특이한 데이터임
이미지
32*32를 찍었다고 하면 1,1에 있는 픽셀과 2,2에 있는 픽셀은 어느정도 연속성이 있음.
이미지나 음성신호는 토폴로지(topology)가 있음
topology : 연속성

컴퓨터가 단어를 이해할 수 있도록 단어를 바꿔주는 것이 word embedding

##### 전통적인 word representation

보통 문서를 가지고 있다고 하면 문서에 있는 모든 단어를 가지고 dictionary를 만듬

one-hot encoding의 문제

sparse 함(정보가 dense 하지 않음)

오직 한 비트만 1이니까벡터가 아무런 의미를 가지지 못함
차원이 많아지면 대부분 점들의 거리가 다 커지고 거리가 의미가 무의미해짐.
아주 가까운거 몇개랑 나머지는 다 멀게 되어버림.
이런식으로 인코딩을 하면 벡터를 가지고 아무런 의미를 얻어낼 수가 없음

#### Neural Word Embeddings

2003년에 나온 NNLM이 시초
비슷한 의미를 가지는 단어는 벡터 공간상에서도 비슷한 위치에 있도록 학습을 하겠다
한단어의 주변에 있는 단어들과 비슷하다고 가정을 하고 학습을 시킴. 
요즘의 컴퓨팅 파워를 사용해서 주변의 더 여러개의 단어를 보면 더 좋을 수 있음

앞의 4개의 단어를 보고 5번째 단어를 맞추겠다
앞의 4개의 단어와 비슷한 의미를 가질 것이라고 가정을 하고 뉴럴 네트워크로 학습함
뉴럴넷은 마지막에 타겟이 되는 벡터와 아웃풋이 정해지면 그 차이를 최대한 줄이기 위해서 학습을 함
4개의 단어를 모두 50차원으로 바꿔서 200차원의 벡터를 만든다음, 나를 표현하는 50차원의 벡터를 잘 맞추는지 본다. 
비슷한 맥락에 나온 단어들은 벡터공간에서 비슷하게 위치하게 됨.
이것을 이용하면 챗봇 개발할 때에는 언어 생성 측면에서는 어려운 면이 많이 생김
모든 단어를 공간상에 맵핑시키는 것이기 때문에, 이 단어가 어떤 단어인지 일대일 단어로 맵핑하는 것이 아니라서 의미를 알기가 어렵다.

#### Word2Vec

앞에와 다르게 앞 4개 단어가 아닌 앞뒤 2개의 단어를 보고 나를 맞추기
컴퓨테이션 cost를 줄임

5.png

##### C-BOW

주위를가지고 날 맞추기

##### Skip-gram

input은 나 단어 하나
나를 보고 그 전에는 뭐가 나왔고, 그 전전에는 뭐가 나왔는지 예측을 한다음, 그걸 잘 맞췄는지를 object function으로 해서 학습시키는 모델이 있었음
이게 대용량 데이터에서는 좀더 성능이 잘 나온다고 알려져 있음
지금은 이런것들을 전처리처럼 다 가져다 씀

#### Glove

얘는 뉴럴네트워크를 이용한 것이아님
문서를 보고 word끼리 문장 내에서 얼마나 자주 같이 나오는지(co-appearance matrix)를 구축을 함. 
그걸 decomposition 해서 word representation을 구축함 